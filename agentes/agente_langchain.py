import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.schema import SystemMessage, HumanMessage, AIMessage
from langgraph.graph import StateGraph, END
from typing import TypedDict, Dict

# Carregar vari√°veis do ambiente (.env)
load_dotenv(os.path.join(os.path.dirname(__file__), "../config/.env"))
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# Verifica√ß√£o da chave da API
if not OPENAI_API_KEY:
    print("‚ö†Ô∏è Erro: A API Key da OpenAI n√£o foi encontrada! Verifique o arquivo .env.")
    exit(1)

# Criando o Modelo de IA com LangChain
modelo = ChatOpenAI(model_name="gpt-4", openai_api_key=OPENAI_API_KEY)

# Criando a Mem√≥ria Persistente com LangGraph
memoria = ChatMessageHistory()

# Criando o Prompt do Agente
prompt = ChatPromptTemplate.from_messages([
    SystemMessage(content="Voc√™ √© um assistente √∫til e amig√°vel."),
    MessagesPlaceholder(variable_name="chat_history"),
    ("user", "{question}")
])

# Definindo o Estado do Agente para LangGraph
class AgentState(TypedDict):
    chat_history: ChatMessageHistory
    question: str
    response: str

# Criando o Grafo do Agente (LangGraph)
workflow = StateGraph(AgentState)

# Fun√ß√£o de Processamento do Agente
def processar_pergunta(state: AgentState) -> Dict:
    """Processa a pergunta e mant√©m a mem√≥ria persistente"""
    # Criando hist√≥rico formatado para o modelo
    mensagens = memoria.messages + [HumanMessage(content=state["question"])]

    # Gerando resposta
    resposta = modelo.invoke(mensagens)

    # Armazenando na mem√≥ria
    memoria.add_user_message(state["question"])
    memoria.add_ai_message(resposta.content)

    return {"chat_history": memoria, "question": state["question"], "response": resposta.content}

# Adicionando estados ao LangGraph
workflow.add_node("responder", processar_pergunta)
workflow.set_entry_point("responder")
workflow.add_edge("responder", END)

# Compilando o Grafo do Agente
agente = workflow.compile()

def iniciar_chat():
    """Inicia o chat interativo com mem√≥ria persistente"""
    print("‚úÖ Agente Inteligente com LangGraph iniciado! (Vers√£o Corrigida)")
    print("Digite 'sair' para encerrar ou 'limpar' para resetar a mem√≥ria.\n")

    while True:
        pergunta = input("Voc√™: ")

        if pergunta.lower() in ["sair", "exit", "fechar"]:
            print("Encerrando o agente...")
            break
        elif pergunta.lower() in ["limpar", "resetar"]:
            memoria.clear()
            print("üß† Mem√≥ria apagada! O agente esqueceu a conversa anterior.")
            continue

        resposta = agente.invoke({"chat_history": memoria, "question": pergunta})

        if "response" in resposta:
            print("Agente:", resposta["response"])
        else:
            print("‚ö†Ô∏è Erro: A resposta n√£o foi gerada corretamente.")

# Executa o agente no terminal
if __name__ == "__main__":
    iniciar_chat()
