import os
import sys
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.schema import SystemMessage, HumanMessage
from langgraph.graph import StateGraph, END
from typing import TypedDict, Dict

# Ajustando o caminho para garantir que o Python encontre database.db_handler
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

from database.db_handler import salvar_memoria, recuperar_memoria  # Importando o banco de dados

# ğŸ”¹ Carregar variÃ¡veis do ambiente (.env)
load_dotenv(os.path.join(os.path.dirname(__file__), "../config/.env"))
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# ğŸ”¹ VerificaÃ§Ã£o da chave da API
if not OPENAI_API_KEY:
    print("âš ï¸ Erro: A API Key da OpenAI nÃ£o foi encontrada! Verifique o arquivo .env.")
    exit(1)

# ğŸ”¹ Criando o Modelo de IA com LangChain
modelo = ChatOpenAI(model_name="gpt-4", openai_api_key=OPENAI_API_KEY)

# ğŸ”¹ Criando a MemÃ³ria Persistente com SQLite
memoria = ChatMessageHistory()

# ğŸ”¹ Criando o Prompt do Agente
prompt = ChatPromptTemplate.from_messages([
    SystemMessage(content="VocÃª Ã© um assistente Ãºtil e memoriza conversas anteriores."),
    MessagesPlaceholder(variable_name="chat_history"),
    ("user", "{question}")
])

# ğŸ”¹ DefiniÃ§Ã£o do Estado do Agente para LangGraph
class AgentState(TypedDict):
    chat_history: ChatMessageHistory
    question: str
    response: str

# ğŸ”¹ Criando o Grafo do Agente (LangGraph)
workflow = StateGraph(AgentState)

# ğŸ”¹ FunÃ§Ã£o de Processamento do Agente
def processar_pergunta(state: AgentState) -> Dict:
    """Processa a pergunta do usuÃ¡rio, mantÃ©m a memÃ³ria persistente e interage com a IA."""
    
    pergunta_usuario = state["question"]
    
    # ğŸ”¹ ObtÃ©m histÃ³rico armazenado no banco de dados SQLite
    historico_salvo = recuperar_memoria()
    
    # ğŸ”¹ Adiciona a conversa atual ao histÃ³rico
    mensagens = historico_salvo + [HumanMessage(content=pergunta_usuario)]

    # ğŸ”¹ Gera resposta da IA
    resposta = modelo.invoke(mensagens)

    # ğŸ”¹ Armazena na memÃ³ria persistente
    salvar_memoria(pergunta_usuario, resposta.content)

    return {"chat_history": memoria, "question": pergunta_usuario, "response": resposta.content}

# ğŸ”¹ Adicionando estados ao LangGraph
workflow.add_node("responder", processar_pergunta)
workflow.set_entry_point("responder")
workflow.add_edge("responder", END)

# ğŸ”¹ Compilando o Grafo do Agente
agente = workflow.compile()

def iniciar_chat():
    """Inicia o chat interativo com memÃ³ria persistente no SQLite."""
    print("âœ… Agente Inteligente com MemÃ³ria iniciado! (LangChain + SQLite)")
    print("Digite 'sair' para encerrar ou 'limpar' para resetar a memÃ³ria.\n")

    while True:
        pergunta = input("VocÃª: ")

        if pergunta.lower() in ["sair", "exit", "fechar"]:
            print("Encerrando o agente...")
            break
        
        elif pergunta.lower() in ["limpar", "resetar"]:
            salvar_memoria("MemÃ³ria resetada", "HistÃ³rico apagado")  # MantÃ©m um registro do reset
            print("ğŸ§  MemÃ³ria apagada! O agente esqueceu a conversa anterior.")
            continue

        resposta = agente.invoke({"chat_history": memoria, "question": pergunta})

        if "response" in resposta:
            print("Agente:", resposta["response"])
        else:
            print("âš ï¸ Erro: A resposta nÃ£o foi gerada corretamente.")

# ğŸ”¹ Executa o agente no terminal
if __name__ == "__main__":
    iniciar_chat()
