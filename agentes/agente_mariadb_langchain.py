import os
import sys
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.schema import SystemMessage, HumanMessage
from langgraph.graph import StateGraph, END
from typing import TypedDict, Dict


# Agora importa o mÃ³dulo do banco de dados corretamente
from db_mariadb import salvar_mensagem, recuperar_historico


# ğŸ”¹ Carregar variÃ¡veis do ambiente (.env)
load_dotenv(os.path.join(os.path.dirname(__file__), "../config/.env"))
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# ğŸ”¹ VerificaÃ§Ã£o da chave de API
if not OPENAI_API_KEY:
    print("âš ï¸ Erro: A API Key da OpenAI nÃ£o foi encontrada! Verifique o arquivo .env.")
    exit(1)

# ğŸ”¹ Criando o modelo de IA utilizando a API da OpenAI via LangChain
modelo = ChatOpenAI(model_name="gpt-4", openai_api_key=OPENAI_API_KEY)

# ğŸ”¹ Criando a MemÃ³ria Persistente
memoria = ChatMessageHistory()

# ğŸ”¹ Criando o Prompt do Agente
prompt = ChatPromptTemplate.from_messages([
    SystemMessage(content="VocÃª Ã© um assistente amigÃ¡vel que lembra das conversas."),
    MessagesPlaceholder(variable_name="chat_history"),
    ("user", "{question}")  # Placeholder para entrada do usuÃ¡rio
])

# ğŸ”¹ DefiniÃ§Ã£o do Estado do Agente
class AgentState(TypedDict):
    chat_history: ChatMessageHistory
    question: str
    response: str

# ğŸ”¹ Criando o Grafo do Agente
workflow = StateGraph(AgentState)

def processar_pergunta(state: AgentState) -> Dict:
    """Processa a entrada do usuÃ¡rio, interage com LangChain e armazena a conversa no MariaDB."""
    
    pergunta_usuario = state["question"]

    # ğŸ” Recupera histÃ³rico da conversa do banco de dados
    historico = recuperar_historico()

    # ğŸ”¹ Adiciona mensagens anteriores do usuÃ¡rio ao contexto
    mensagens = historico + [HumanMessage(content=pergunta_usuario)]

    # ğŸ”¹ ObtÃ©m resposta da IA
    resposta = modelo.invoke(mensagens)

    # ğŸ”¹ Armazena no banco de dados
    salvar_mensagem("user", pergunta_usuario)
    salvar_mensagem("ai", resposta.content)

    return {"chat_history": memoria, "question": pergunta_usuario, "response": resposta.content}

# ğŸ”¹ ConfiguraÃ§Ã£o do LangGraph
workflow.add_node("responder", processar_pergunta)
workflow.set_entry_point("responder")
workflow.add_edge("responder", END)

# ğŸ”¹ Compilando o Grafo do Agente
agente = workflow.compile()

def iniciar_chat():
    """Inicia o chat interativo com memÃ³ria persistente no MariaDB."""
    
    print("âœ… Agente Inteligente com LangGraph + MariaDB iniciado!")
    print("Digite 'sair' para encerrar ou 'limpar' para resetar a memÃ³ria.\n")

    while True:
        pergunta = input("VocÃª: ")

        if pergunta.lower() in ["sair", "exit", "fechar"]:
            print("Encerrando o agente...")
            break
        elif pergunta.lower() in ["limpar", "resetar"]:
            print("âš ï¸ FunÃ§Ã£o de limpeza de histÃ³rico ainda nÃ£o implementada!")
            continue

        resposta = agente.invoke({"chat_history": memoria, "question": pergunta})

        if "response" in resposta:
            print("Agente:", resposta["response"])
        else:
            print("âš ï¸ Erro: A resposta nÃ£o foi gerada corretamente.")

# ğŸ”¹ Executa o agente
if __name__ == "__main__":
    iniciar_chat()
