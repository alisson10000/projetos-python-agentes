import os
import requests
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.schema import SystemMessage, HumanMessage
from langgraph.graph import StateGraph, END
from typing import TypedDict, Dict
from clima_langchain import obter_previsao_tempo  # FunÃ§Ã£o que busca a previsÃ£o do tempo

# ğŸ”¹ Carregar variÃ¡veis do ambiente (.env) contendo as chaves das APIs
load_dotenv(os.path.join(os.path.dirname(__file__), "../config/.env"))
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# ğŸ”¹ ValidaÃ§Ã£o da chave de API para evitar erros na execuÃ§Ã£o
if not OPENAI_API_KEY:
    print("âš ï¸ Erro: A API Key da OpenAI nÃ£o foi encontrada! Verifique o arquivo .env.")
    exit(1)

# ğŸ”¹ Criando o modelo de IA utilizando a API da OpenAI via LangChain
modelo = ChatOpenAI(model_name="gpt-4", openai_api_key=OPENAI_API_KEY)

# ğŸ”¹ Criando a MemÃ³ria Persistente para manter o histÃ³rico da conversa
memoria = ChatMessageHistory()

# ğŸ”¹ Criando um prompt personalizado para o agente, permitindo a integraÃ§Ã£o do histÃ³rico da conversa
prompt = ChatPromptTemplate.from_messages([
    SystemMessage(content="VocÃª Ã© um assistente Ãºtil e amigÃ¡vel. Se perguntarem sobre o tempo, consulte a API de clima."),
    MessagesPlaceholder(variable_name="chat_history"),
    ("user", "{question}")  # Placeholder para a pergunta do usuÃ¡rio
])

# ğŸ”¹ DefiniÃ§Ã£o da estrutura do estado do agente usando TypedDict
class AgentState(TypedDict):
    chat_history: ChatMessageHistory  # HistÃ³rico das interaÃ§Ãµes do usuÃ¡rio com o agente
    question: str                     # Pergunta atual do usuÃ¡rio
    response: str                      # Resposta gerada pelo agente

# ğŸ”¹ Criando um fluxo de execuÃ§Ã£o para o agente usando LangGraph
workflow = StateGraph(AgentState)

# ğŸ”¹ FunÃ§Ã£o principal do agente para processar perguntas e gerar respostas
def processar_pergunta(state: AgentState) -> Dict:
    """Processa a entrada do usuÃ¡rio, identifica se Ã© uma consulta sobre clima ou um diÃ¡logo geral, 
    mantÃ©m o histÃ³rico e gera respostas adequadas."""

    pergunta_usuario = state["question"].lower()

    # âœ… ğŸ” Verifica se a pergunta estÃ¡ relacionada ao clima usando palavras-chave comuns
    if any(palavra in pergunta_usuario for palavra in ["tempo", "previsÃ£o", "clima", "vai chover", "chuva", "sol", "temperatura"]):
        resposta_clima = obter_previsao_tempo(state["question"])  # Chama a API de previsÃ£o do tempo
        return {"chat_history": memoria, "question": state["question"], "response": resposta_clima}

    # âœ… ğŸ” Se nÃ£o for uma consulta sobre clima, a IA responde normalmente
    mensagens = memoria.messages + [HumanMessage(content=state["question"])]  # Atualiza o histÃ³rico com a nova pergunta

    # âœ… ğŸ” Gera uma resposta baseada no contexto da conversa
    resposta = modelo.invoke(mensagens)

    # âœ… ğŸ” Armazena a interaÃ§Ã£o na memÃ³ria para manter a continuidade do diÃ¡logo
    memoria.add_user_message(state["question"])
    memoria.add_ai_message(resposta.content)

    # Retorna o novo estado atualizado contendo a resposta
    return {"chat_history": memoria, "question": state["question"], "response": resposta.content}

# ğŸ”¹ Adicionando estados ao fluxo do LangGraph
workflow.add_node("responder", processar_pergunta)  # Define o nÃ³ de resposta
workflow.set_entry_point("responder")  # Define o ponto de entrada no fluxo
workflow.add_edge("responder", END)  # Define a transiÃ§Ã£o final do fluxo

# ğŸ”¹ Compilando o fluxo do agente para ser executado
agente = workflow.compile()

def iniciar_chat():
    """Inicia a interaÃ§Ã£o do usuÃ¡rio com o agente de IA, permitindo perguntas gerais e consultas sobre clima,
    com suporte a memÃ³ria de contexto e histÃ³rico da conversa."""

    print("âœ… Agente Inteligente com LangGraph iniciado! (VersÃ£o Final)")
    print("Digite 'sair' para encerrar ou 'limpar' para resetar a memÃ³ria.\n")

    while True:
        pergunta = input("VocÃª: ")

        # âœ… ğŸ” Verifica se o usuÃ¡rio quer encerrar o chat
        if pergunta.lower() in ["sair", "exit", "fechar"]:
            print("Encerrando o agente...")
            break
        
        # âœ… ğŸ” Verifica se o usuÃ¡rio quer limpar a memÃ³ria
        elif pergunta.lower() in ["limpar", "resetar"]:
            memoria.clear()
            print("ğŸ§  MemÃ³ria apagada! O agente esqueceu a conversa anterior.")
            continue

        # âœ… ğŸ” Processa a pergunta do usuÃ¡rio e obtÃ©m a resposta do agente
        resposta = agente.invoke({"chat_history": memoria, "question": pergunta})

        # âœ… ğŸ” Exibe a resposta do agente ou um aviso em caso de erro
        if "response" in resposta:
            print("Agente:", resposta["response"])
        else:
            print("âš ï¸ Erro: A resposta nÃ£o foi gerada corretamente.")

# ğŸ”¹ Executa o agente no terminal quando o script for executado diretamente
if __name__ == "__main__":
    iniciar_chat()
